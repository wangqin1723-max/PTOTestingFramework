"""
Orchestration code generator for PTO testing framework.

Generates orchestration C++ code that builds the task graph
for simpler runtime execution.
"""

from typing import Any, Dict, List, TYPE_CHECKING

if TYPE_CHECKING:
    from pto_test.core.test_case import TensorSpec


class OrchGenerator:
    """Generates orchestration C++ code for simpler runtime.

    The generated function has the signature:
        extern "C" int build_test_graph(Runtime* runtime, uint64_t* args, int arg_count)

    It handles:
    - Extracting host pointers and sizes from args
    - Allocating device memory
    - Copying inputs to device
    - Recording outputs for copy-back
    - Providing a placeholder for user-defined task creation

    Task creation and dependency setup must be implemented by the user.
    Override get_orchestration() in your test case to provide custom task logic.

    Example:
        generator = OrchGenerator(function_name="build_test_graph")
        code = generator.generate(tensor_specs, kernel_configs)
    """

    def __init__(self, function_name: str = "build_test_graph"):
        """Initialize orchestration generator.

        Args:
            function_name: Name of the generated function (default: "build_test_graph").
        """
        self.function_name = function_name

    def generate(
        self,
        tensor_specs: List["TensorSpec"],
        kernel_configs: List[Dict[str, Any]],
    ) -> str:
        """Generate orchestration C++ source code.

        Generates an orchestration skeleton that:
        1. Extracts N tensor pointers + N sizes + 1 element count from args
        2. Allocates device memory for each tensor
        3. Copies inputs to device
        4. Records outputs for copy-back
        5. Provides a placeholder for user-defined task creation

        Args:
            tensor_specs: Ordered list of TensorSpec (inputs first, outputs last).
            kernel_configs: Kernel config dicts from KernelGenerator.

        Returns:
            Complete C++ orchestration source code string with task creation placeholder.
        """
        n_tensors = len(tensor_specs)
        expected_args = 2 * n_tensors + 1  # ptrs + sizes + count

        # Identify inputs and outputs
        input_specs = [t for t in tensor_specs if not t.is_output]
        output_specs = [t for t in tensor_specs if t.is_output]

        # Build the code sections
        parts = []

        # Header
        parts.append(self._generate_header())

        # Function signature
        parts.append(f'extern "C" {{\n')
        parts.append(f"\nint {self.function_name}(Runtime* runtime, uint64_t* args, int arg_count) {{\n")

        # Argument validation
        parts.append(self._generate_arg_validation(n_tensors, expected_args))

        # Extract arguments
        parts.append(self._generate_arg_extraction(tensor_specs))

        # Allocate device memory and copy inputs
        parts.append(self._generate_memory_allocation(tensor_specs, input_specs, output_specs))

        # Build task(s)
        parts.append(self._generate_task_creation(tensor_specs, kernel_configs))

        # Return
        parts.append("    return 0;\n")
        parts.append("}\n")
        parts.append("\n}  // extern \"C\"\n")

        return "".join(parts)

    def _generate_header(self) -> str:
        """Generate file header with includes."""
        return '''/**
 * Auto-generated Orchestration Function
 *
 * Generated by pto_test.codegen.OrchGenerator
 */

#include "runtime.h"
#include <iostream>

'''

    def _generate_arg_validation(self, n_tensors: int, expected_args: int) -> str:
        """Generate argument count validation."""
        return f'''    // Validate argument count
    // Expected: [{", ".join([f"ptr_{i}" for i in range(n_tensors)])}, {", ".join([f"size_{i}" for i in range(n_tensors)])}, count]
    if (arg_count < {expected_args}) {{
        std::cerr << "{self.function_name}: Expected at least {expected_args} args, got " << arg_count << '\\n';
        return -1;
    }}

'''

    def _generate_arg_extraction(self, tensor_specs: List["TensorSpec"]) -> str:
        """Generate argument extraction code."""
        lines = ["    // Extract arguments\n"]
        n = len(tensor_specs)

        # Extract pointers
        for i, spec in enumerate(tensor_specs):
            lines.append(f"    void* host_{spec.name} = reinterpret_cast<void*>(args[{i}]);\n")

        lines.append("\n")

        # Extract sizes
        for i, spec in enumerate(tensor_specs):
            lines.append(f"    size_t size_{spec.name} = static_cast<size_t>(args[{n + i}]);\n")

        lines.append("\n")

        # Extract element count
        lines.append(f"    int element_count = static_cast<int>(args[{2 * n}]);\n")
        lines.append("\n")

        return "".join(lines)

    def _generate_memory_allocation(
        self,
        tensor_specs: List["TensorSpec"],
        input_specs: List["TensorSpec"],
        output_specs: List["TensorSpec"],
    ) -> str:
        """Generate device memory allocation and input copy code with proper cleanup."""
        lines = ["    // Allocate device memory\n"]

        all_specs = input_specs + output_specs

        for i, spec in enumerate(all_specs):
            is_output = spec.is_output

            # Allocate memory
            lines.append(f"    void* dev_{spec.name} = runtime->host_api.device_malloc(size_{spec.name});\n")
            lines.append(f"    if (!dev_{spec.name}) {{\n")
            lines.append(f'        std::cerr << "Error: Failed to allocate device memory for {spec.name}\\n";\n')

            # Cleanup all previously allocated tensors
            for j in range(i):
                prev_spec = all_specs[j]
                lines.append(f"        runtime->host_api.device_free(dev_{prev_spec.name});\n")

            lines.append("        return -1;\n")
            lines.append("    }\n")

            # Copy to device for inputs or record for outputs
            if is_output:
                lines.append(f"    // Record output tensor for copy-back during finalize\n")
                lines.append(f"    runtime->record_tensor_pair(host_{spec.name}, dev_{spec.name}, size_{spec.name});\n")
            else:
                lines.append(f"    runtime->host_api.copy_to_device(dev_{spec.name}, host_{spec.name}, size_{spec.name});\n")

            lines.append("\n")

        return "".join(lines)

    def _generate_task_creation(
        self,
        tensor_specs: List["TensorSpec"],
        kernel_configs: List[Dict[str, Any]],
    ) -> str:
        """Generate placeholder comment for user-defined task creation.

        The user should implement their own task creation and dependency logic here.
        """
        lines = []
        lines.append("    // ========================================\n")
        lines.append("    // TODO: Add your task creation code here\n")
        lines.append("    // ========================================\n")
        lines.append("    //\n")
        lines.append("    // Available device tensors:\n")

        # List all available device tensors for reference
        for spec in tensor_specs:
            tensor_type = "output" if spec.is_output else "input"
            lines.append(f"    //   - dev_{spec.name} ({tensor_type})\n")

        lines.append("    //   - element_count (size)\n")
        lines.append("    //\n")
        lines.append("    // Example task creation:\n")
        lines.append("    //   uint64_t args_t0[N];\n")
        lines.append("    //   args_t0[0] = reinterpret_cast<uint64_t>(dev_input);\n")
        lines.append("    //   args_t0[1] = reinterpret_cast<uint64_t>(dev_output);\n")
        lines.append("    //   args_t0[2] = element_count;\n")
        lines.append("    //   int t0 = runtime->add_task(args_t0, N, func_id, 1);\n")
        lines.append("    //\n")
        lines.append("    // Example dependency:\n")
        lines.append("    //   runtime->add_successor(t0, t1);  // t0 â†’ t1\n")
        lines.append("    //\n")

        if kernel_configs:
            lines.append("    // Available kernel func_ids:\n")
            for i, cfg in enumerate(kernel_configs):
                func_id = cfg.get("func_id", i)
                kernel_name = cfg.get("name", f"kernel_{i}")
                lines.append(f"    //   - func_id={func_id}: {kernel_name}\n")
            lines.append("    //\n")

        lines.append("    // ========================================\n")
        lines.append("\n")

        return "".join(lines)


class MultiKernelOrchGenerator(OrchGenerator):
    """Extended orchestration generator for multi-kernel test cases.

    This generator can handle more complex task graphs with multiple
    kernels and dependencies between them. Users can specify the
    task graph structure explicitly.

    Note: This is for advanced use cases. Most single-kernel tests
    should use the base OrchGenerator.
    """

    def generate_with_tasks(
        self,
        tensor_specs: List["TensorSpec"],
        tasks: List[Dict[str, Any]],
        dependencies: List[tuple],
    ) -> str:
        """Generate orchestration with explicit task graph.

        Args:
            tensor_specs: Tensor specifications.
            tasks: List of task definitions:
                   [{"kernel_id": 0, "inputs": ["a", "b"], "outputs": ["c"]}, ...]
            dependencies: List of (predecessor_idx, successor_idx) tuples.

        Returns:
            C++ orchestration source code.
        """
        # TODO: Implement multi-kernel task graph generation
        raise NotImplementedError(
            "Multi-kernel orchestration not yet implemented. "
            "Override get_orchestration() in your test case for complex cases."
        )
